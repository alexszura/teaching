# A Machine Learning & Data Science Companion Reading List
This is a collection of papers, articles, and blog posts that are great for reading alongside a rigorous introductory course in machine learning and/or data science (with mild focus on computational biology).

1. [Data science workflow: overview & challenges](https://m.cacm.acm.org/blogs/blog-cacm/169199-data-science-workflow-overview-and-challenges/fulltext) | Philip Guo  
> "Four main phases: preparation of the data, alternating between running the analysis and reflection to interpret the outputs, and finally dissemination of results in the form of written reports and/or executable code."

2. [A few useful things to know about machine learning](https://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf) | Pedro Domingos  
> "Twelve key lessons that machine learning researchers and practitioners have learned. These include pitfalls to avoid, important issues to focus on, and answers to common questions."

3. [Practical advice for analysis of large, complex data sets](http://www.unofficialgoogledatascience.com/2016/10/practical-advice-for-analysis-of-large.html) | Patrick Riley  
> "Technical: Ideas and techniques for how to manipulate and examine your data. Process: Recommendations on how you approach your data, what questions to ask, and what things to check. Social: How to work with others and communicate about your data and insights."

4. [Rules of Machine Learning: Best Practices for ML Engineering](http://martin.zinkevich.org/rules_of_ml/rules_of_ml.pdf) | Martin Zinkevich  
> "This document is intended to help those with a basic knowledge of machine learning get the benefit of best practices in ML from around Google."

5. [Ten quick tips for machine learning in computational biology](https://biodatamining.biomedcentral.com/articles/10.1186/s13040-017-0155-3) | Davide Chicco  
> "Ten quick tips to take advantage of ML in any computational biology context, by avoiding some common errors that we observed hundreds of times in multiple bioinformatics projects."

6. [Machine learning applications in genetics and genomics](http://www.nature.com/nrg/journal/v16/n6/abs/nrg3920.html) | Maxwell W. Libbrecht, William Stafford Noble  
> "[O]verview of ML applications for the analysis of genome sequencing data sets, including the annotation of sequence elements and epigenetic, proteomic or metabolomic data; Considerations and recurrent challenges in the application of supervised, semi-supervised and unsupervised ML methods, as well as of generative and discriminative modelling approaches; General guidelines to assist in the selection of these ML methods and their practical application for the analysis of genetic and genomic data sets."

7. [A review on machine learning principles for multi-view biological data integration](http://bib.oxfordjournals.org/content/early/2016/11/28/bib.bbw113.full) | Yifeng Li, Fang-Xiang Wu, Alioune Ngom  
> "Review on omics and clinical data integration techniques, from a ML perspective, for various analyses such as prediction, clustering, dimension reduction and association. We shall show that Bayesian models are able to use prior information and model measurements with various distributions; tree-based methods can either build a tree with all features or collectively make a final decision based on trees learned from each view; kernel methods fuse the similarity matrices learned from individual views together for a final similarity matrix or learning model; network-based fusion methods are capable of inferring direct and indirect associations in a heterogeneous network; matrix factorization models have potential to learn interactions among features from different views; and a range of deep neural networks can be integrated in multi-modal learning for capturing the complex mechanism of biological systems."

8. [Data-driven Advice for Applying Machine Learning to Bioinformatics Problems](https://arxiv.org/abs/1708.05070) | Randal Olson, William La Cava, Zairah Mustahsan, Akshay Varik, Jason Moore  
> "Analysis of 13 state-of-the-art, commonly used ML algorithms on a set of 165 publicly available classification problems; Recommendation of five algorithms with hyperparameters that maximize classifier performance across the tested problems, as well as general guidelines for applying machine learning to supervised classification problems."  
> A good open-review of this manuscript is [here](https://crossinvalidation.com/2017/08/22/quantitative-comparison-of-scikit-learns-predictive-models-on-a-large-number-of-machine-learning-datasets-a-good-start/).

9. [Do We Need Hundreds of Classifiers to Solve Real World Classification Problems?](https://dl.acm.org/citation.cfm?id=2697065) | Manuel Fernández-Delgado, Eva Cernadas, Senén Barro, Dinani Amorim  
> "We evaluate 179 classifiers arising from 17 families ... [using] 121 data sets. The random forest is clearly the best family of classifiers, followed by SVM (not statisitically significant difference from RF), neural networks, and boosting ensembles."

10. [I tried a bunch of things: the dangers of unexpected overfitting in classification](http://biorxiv.org/content/biorxiv/early/2016/10/03/078816) | Michael Skocik, John Collins, Chloe Callahan-Flintoft, Howard Bowman, Brad Wyble  
> "[We] demonstrate the ease by which overfitting can occur, despite the use of cross validation. We demonstrate that comparable and non-generalizable results can be obtained on informative and non-informative (i.e. random) data by iteratively modifying hyperparameters in seemingly innocuous ways. We recommend a number of techniques for limiting overfitting, such as lock boxes, blind analyses, and pre-registrations."

11. [Machine learning: the high interest credit card of technical debt](https://research.google.com/pubs/pub43146.html) | D. Sculley, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar Ebner, Vinay Chaudhary, Michael Young  
> "It is remarkably easy to incur massive ongoing maintenance costs at the system level when applying ML (quick wins don't come for free). [We present] several ML-specific risk factors and design patterns to be avoided or refactored where possible: e.g., boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, changes in the external world, and a variety of system-level anti-patterns."

12. [Excellent notes on statistics, data science, machine learning, and programming](https://chrisalbon.com/) | Chris Albon  
> Concise, informative snippets on everything from regression assumptions to distributed computing.

13. [Excellent glossary of machine learning terms & concepts](https://developers.google.com/machine-learning/glossary/)  
> Definitions of general ML terms.
